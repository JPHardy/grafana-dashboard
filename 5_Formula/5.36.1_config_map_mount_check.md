2. **Verify Volume Mount in Pod**:
  
  @rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ kubectl get pods -n grafana-monitoring
NAME                       READY   STATUS    RESTARTS   AGE
grafana-684556c4c8-6nvng   1/1     Running   0          8m39s
  
   Ensure the ConfigMap is mounted correctly. Check the directory in the Grafana pod:
   ```bash
   kubectl exec -it <grafana-pod-name> -n grafana-monitoring -- ls /opt/bitnami/grafana/dashboards
   kubectl exec -it grafana-684556c4c8-6nvng -n grafana-monitoring -- ls /opt/bitnami/grafana/dashboards
   ```


   @rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ kubectl exec -it grafana-684556c4c8-6nvng -n grafana-monitoring -- ls /opt/bitnami/grafana/dashboards
ls: cannot access '/opt/bitnami/grafana/dashboards': No such file or directory
command terminated with exit code 2
@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ 

It looks like there are a few issues that need addressing:

### 1. **Missing `/opt/bitnami/grafana/dashboards` Directory**
   - The error `ls: cannot access '/opt/bitnami/grafana/dashboards': No such file or directory` suggests that the directory for dashboards doesn't exist. This could be due to how the volumes are mounted in the Grafana pod.

### 2. **Errors Deleting Resources**
   - It seems there were some issues with the `kubectl delete` commands, mainly due to syntax or missing parameters.

---

### Steps to Resolve

1. **Verify Grafana Pod Volumes**:
   Ensure that the Grafana pod has the correct volume mount to the dashboard configuration. Check the deployment or stateful set of the Grafana pod to confirm that the ConfigMap is mounted correctly.

   You can get the pod's details using:
   ```bash
   kubectl get pod grafana-684556c4c8-6nvng -n grafana-monitoring -o yaml
   ```

   Look under the `volumes` section to confirm that the volume from `grafana-dashboard-config` is mounted at `/opt/bitnami/grafana/dashboards`.

2. **Check ConfigMap Mounting**:
   If the volume is correctly mounted, but the directory still doesn't exist, you may need to modify the Grafana configuration to ensure it creates the `/dashboards` directory when it's missing.

3. **Delete Pods Correctly**:
   To delete the pods properly and restart them, use the following command:

   ```bash
   kubectl delete pod -n grafana-monitoring -l app.kubernetes.io/name=grafana
   ```

   This command deletes the pods based on the label selector for Grafana. This should trigger the creation of new pods with the updated configurations.

4. **Check Grafana Logs**:
   After deleting and restarting the pod, check the logs again to see if there are any error messages related to dashboard provisioning:
   ```bash
   kubectl logs -f <grafana-pod-name> -n grafana-monitoring
   ```

5. **Ensure the Correct Configuration**:
   Reconfirm the ConfigMap's JSON structure and make sure it's structured for Grafana provisioning as discussed earlier.

Let me know if you encounter any further issues!


@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ kubectl delete pod grafana-684556c4c8-6nvng -n grafana-monitoring
pod "grafana-684556c4c8-6nvng" deleted
@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ 

@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ kubectl delete pod grafana-684556c4c8-6nvng -n grafana-monitoring
pod "grafana-684556c4c8-6nvng" deleted
@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ kubectl get pods -n grafana-monitoring
NAME                       READY   STATUS    RESTARTS   AGE
grafana-684556c4c8-7hqp5   1/1     Running   0          62s


kubectl exec -it grafana-684556c4c8-7hqp5 -n grafana-monitoring -- ls /opt/bitnami/grafana/dashboards

@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ kubectl exec -it grafana-684556c4c8-7hqp5 -n grafana-monitoring -- ls /opt/bitnami/grafana/dashboards
ls: cannot access '/opt/bitnami/grafana/dashboards': No such file or directory
command terminated with exit code 2
@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ 

@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DashboardConfigMaps (main) $ kubectl exec -it grafana-684556c4c8-7hqp5 -n grafana-monitoring -- ls /opt/bitnami/grafana
bin  conf  conf.default  data  default-plugins  licenses  logs  public  tmp

output:
It seems like the `/opt/bitnami/grafana/dashboards` directory doesn't exist by default in the Grafana pod. This suggests that either:

1. **The volume mount is not set up correctly** to create that directory.
2. **The default Grafana setup doesn't include that directory**.

You can resolve this by manually creating the directory or verifying the Grafana configuration for provisioning dashboards.

### Steps to Resolve:

1. **Manually Create the Directory**:
   You can create the missing directory inside the pod and mount the ConfigMap to that directory.
   Run the following command to create the directory manually in the Grafana pod:
   ```bash
   kubectl exec -it grafana-684556c4c8-7hqp5 -n grafana-monitoring -- mkdir -p /opt/bitnami/grafana/dashboards
   ```

2. **Revisit Volume Mount Configuration**:
   Ensure that your `grafana-dashboard-deployment.yaml` (or Helm values file) has the correct volume mount configuration, like this:
   ```yaml
   volumeMounts:
     - name: grafana-dashboard-config
       mountPath: /opt/bitnami/grafana/dashboards
       readOnly: true
   volumes:
     - name: grafana-dashboard-config
       configMap:
         name: grafana-dashboard-config
   ```

3. **Confirm Volume Mounting**:
   To check whether the ConfigMap is correctly mounted:
   ```bash
   kubectl exec -it grafana-684556c4c8-7hqp5 -n grafana-monitoring -- ls /opt/bitnami/grafana/dashboards
   ```

4. **Restart the Grafana Pod**:
   After making these changes, restart the Grafana pod to apply the new configuration:
   ```bash
   kubectl rollout restart deployment grafana -n grafana-monitoring
   ```

5. **Check Logs**:
   Check the logs for any errors related to dashboard provisioning:
   ```bash
   kubectl logs -f grafana-684556c4c8-7hqp5 -n grafana-monitoring
   ```

Let me know if you need more help!