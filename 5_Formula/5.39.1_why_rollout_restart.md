It looks like the pod configuration is set up with several `emptyDir` volumes for various paths, but there is no indication of the `dashboards` directory being explicitly mounted. The pod's current configuration does not reference the `grafana-dashboard-config` ConfigMap for the dashboards.

### To fix this, follow these steps:

1. **Add a Volume for Dashboards**:
   You need to ensure the `grafana-dashboard-config` ConfigMap is mounted as a volume inside the Grafana container. Update the pod or deployment configuration to include the volume.

   Hereâ€™s an example of how to do that:

   ```yaml
   volumes:
     - name: grafana-dashboard-config
       configMap:
         name: grafana-dashboard-config
   ```

2. **Update the `volumeMounts` for Dashboards**:
   Ensure that the dashboard directory is correctly mounted inside the container. Add the following to the `volumeMounts` section of your pod configuration:

   ```yaml
   volumeMounts:
     - mountPath: /opt/bitnami/grafana/dashboards
       name: grafana-dashboard-config
   ```

3. **Apply Changes**:
   After modifying the configuration, apply the changes to the deployment:
   ```bash
   kubectl apply -f grafana-deployment.yaml
   ```

   If you are using Helm for deployment, you can update the `values.yaml` file and then run:
   ```bash
   helm upgrade grafana bitnami/grafana -f values.yaml -n grafana-monitoring
   ```

4. **Restart the Pod**:
   Restart the pod to ensure the changes take effect:
   ```bash
   kubectl rollout restart deployment grafana -n grafana-monitoring
   ```

After completing these steps, the `dashboards` directory should be populated with your ConfigMap data and available to Grafana. Let me know if you need further assistance!


>>> why roll out restart

Using `kubectl rollout restart` is important because it ensures that the changes you made to the configuration are applied correctly. Here are a few reasons why you might need to use this command:

1. **Apply Configuration Changes**: When you update a ConfigMap or Secret, the changes are not automatically picked up by the running pods. Restarting the pods ensures they use the latest configuration[1](https://kodekloud.com/blog/kubectl-rollout-restart/).

2. **Troubleshooting**: If your application is in an undesirable state or experiencing issues, restarting the pods can help resolve these problems by recreating the pods with the current configuration[2](https://supportfly.io/kubectl-rollout-restart/).

3. **CrashLoopBackOff**: If a pod is stuck in a CrashLoopBackOff state, restarting it can help recover from this state[2](https://supportfly.io/kubectl-rollout-restart/).

The `kubectl rollout restart` command triggers a rolling restart of the pods, ensuring that the application remains available during the process[1](https://kodekloud.com/blog/kubectl-rollout-restart/). This means the old pods are gradually replaced with new ones, minimizing downtime.

If you have any more questions or need further assistance, feel free to ask!