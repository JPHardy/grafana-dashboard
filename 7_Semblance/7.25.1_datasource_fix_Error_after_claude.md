@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main) $ kubectl describe pod grafana-6fbbccf49f-hbnr4  -n grafana-monitoring
Name:             grafana-6fbbccf49f-hbnr4
Namespace:        grafana-monitoring
Priority:         0
Service Account:  grafana
Node:             minikube/192.168.49.2
Start Time:       Tue, 14 Jan 2025 15:53:31 +0000
Labels:           app.kubernetes.io/component=grafana
                  app.kubernetes.io/instance=grafana
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=grafana
                  app.kubernetes.io/version=11.4.0
                  helm.sh/chart=grafana-11.4.2
                  pod-template-hash=6fbbccf49f
Annotations:      checksum/config: 19c20dc5bd07bc522511ffab5d9eac1446dcd3a6450e7bbfdf09a5d562286cec
                  checksum/dashboard-provider: cd54948bf37e0721fd87481725f1618776aa7ba6f4259f9eb094412c9ca9d1f6
                  checksum/secret: 4392d22dc063d7ffda1126edc47da819eefc80a1e3130abeb719c2232887e531
Status:           Running
IP:               10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/grafana-6fbbccf49f
Containers:
  grafana:
    Container ID:    docker://4d974b1c58bb2fa5ecba0f9b03d29b28b14574ce7f0ab874ad4b3e59d0a435b5
    Image:           docker.io/bitnami/grafana:11.4.0-debian-12-r0
    Image ID:        docker-pullable://bitnami/grafana@sha256:cb8ab551567670a68f44052587c169339732fa7e0f4d806aecfd6b1e5889c626
    Port:            3000/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Waiting
      Reason:        CrashLoopBackOff
    Last State:      Terminated
      Reason:        Error
      Exit Code:     1
      Started:       Tue, 14 Jan 2025 15:57:34 +0000
      Finished:      Tue, 14 Jan 2025 15:57:40 +0000
    Ready:           False
    Restart Count:   5
    Limits:
      cpu:                150m
      ephemeral-storage:  2Gi
      memory:             192Mi
    Requests:
      cpu:                100m
      ephemeral-storage:  50Mi
      memory:             128Mi
    Liveness:             tcp-socket :dashboard delay=120s timeout=5s period=10s #success=1 #failure=6
    Readiness:            http-get http://:dashboard/api/health delay=30s timeout=5s period=10s #success=1 #failure=6
    Environment Variables from:
      grafana-envvars  ConfigMap  Optional: false
    Environment:
      GF_SECURITY_ADMIN_PASSWORD:  <set to the key 'GF_SECURITY_ADMIN_PASSWORD' in secret 'grafana-admin'>  Optional: false
    Mounts:
      /bitnami/grafana from empty-dir (rw,path="app-volume-dir")
      /opt/bitnami/grafana/conf from empty-dir (rw,path="app-conf-dir")
      /opt/bitnami/grafana/conf.default/provisioning/dashboards from dashboards-provider (rw)
      /opt/bitnami/grafana/conf.default/provisioning/datasources from datasources (rw)
      /opt/bitnami/grafana/dashboards/grafana_dashboard_config1.json from grafana-dashboard-config1 (rw,path="grafana_dashboard_config1.json")
      /opt/bitnami/grafana/dashboards/grafana_dashboard_config2.json from grafana-dashboard-config2 (rw,path="grafana_dashboard_config2.json")
      /opt/bitnami/grafana/data from data (rw)
      /opt/bitnami/grafana/tmp from empty-dir (rw,path="app-tmp-dir")
      /tmp from empty-dir (rw,path="tmp-dir")
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  empty-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  grafana
    ReadOnly:   false
  dashboards-provider:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-provider
    Optional:  false
  grafana-dashboard-config1:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-config1
    Optional:  false
  grafana-dashboard-config2:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-config2
    Optional:  false
  datasources:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  grafana-datasource-secret
    Optional:    false
  grafana-storage:
    Type:        EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:      
    SizeLimit:   <unset>
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                    From               Message
  ----     ------            ----                   ----               -------
  Warning  FailedScheduling  5m11s (x2 over 5m41s)  default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         5m9s                   default-scheduler  Successfully assigned grafana-monitoring/grafana-6fbbccf49f-hbnr4 to minikube
  Normal   Pulling           5m9s                   kubelet            Pulling image "docker.io/bitnami/grafana:11.4.0-debian-12-r0"
  Normal   Pulled            4m46s                  kubelet            Successfully pulled image "docker.io/bitnami/grafana:11.4.0-debian-12-r0" in 22.057s (22.057s including waiting). Image size: 702398522 bytes.
  Normal   Created           2m38s (x5 over 4m44s)  kubelet            Created container grafana
  Normal   Pulled            2m38s (x4 over 4m29s)  kubelet            Container image "docker.io/bitnami/grafana:11.4.0-debian-12-r0" already present on machine
  Normal   Started           2m37s (x5 over 4m44s)  kubelet            Started container grafana
  Warning  BackOff           5s (x23 over 4m23s)    kubelet            Back-off restarting failed container grafana in pod grafana-6fbbccf49f-hbnr4_grafana-monitoring(6d4bb734-b5ec-454f-a2c3-40625caebd32)
@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main) $ 

Resolution>
Let me help analyze and fix the issues with your Grafana pod that's in CrashLoopBackOff state. There are several key issues to address:

1. First, let's check the pod logs to understand why it's crashing:

```bash
kubectl logs grafana-6fbbccf49f-hbnr4 -n grafana-monitoring
```

2. From the pod description, I can see a few potential issues:

a. PersistentVolume Issue:
```yaml
Warning  FailedScheduling  5m11s (x2 over 5m41s)  default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims
```

Let's create a PersistentVolume and PersistentVolumeClaim:



```yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: grafana-pv
  namespace: grafana-monitoring
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/grafana-storage
  persistentVolumeReclaimPolicy: Retain
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana
  namespace: grafana-monitoring
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

```

3. Apply the storage configuration:
```bash
kubectl apply -f grafana-storage.yaml
```

4. The pod also shows resource constraints that might be too tight. Let's update the deployment with adjusted resources:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: grafana-monitoring
spec:
  template:
    spec:
      containers:
      - name: grafana
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 250m
            memory: 256Mi

```

5. Apply the resource updates:
```bash
kubectl apply -f grafana-deployment-patch.yaml
```

6. Verify permissions on mounted volumes:
```bash
kubectl exec -it grafana-6fbbccf49f-hbnr4 -n grafana-monitoring -- ls -la /bitnami/grafana
```

Additional steps to check:

1. Verify your datasource secret is correctly formatted:
```bash
kubectl get secret grafana-datasource-secret -n grafana-monitoring -o yaml
```

2. Check if ConfigMaps are properly created:
```bash
kubectl get configmap -n grafana-monitoring
```

3. Delete the pod to force a fresh deployment:
```bash
kubectl delete pod grafana-6fbbccf49f-hbnr4 -n grafana-monitoring
```

If the issue persists after applying these fixes, please share the pod logs so I can provide more specific guidance. The logs will show the exact error causing the crashes.

Would you like me to explain any of these steps in more detail or help with interpreting the logs?

---

Claude output

@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main) $ kubectl logs grafana-6fbbccf49f-hbnr4 -n grafana-monitoring
grafana 16:00:31.12 INFO  ==> 
grafana 16:00:31.12 INFO  ==> Welcome to the Bitnami grafana container
grafana 16:00:31.12 INFO  ==> Subscribe to project updates by watching https://github.com/bitnami/containers
grafana 16:00:31.12 INFO  ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
grafana 16:00:31.12 INFO  ==> 
grafana 16:00:31.22 INFO  ==> ** Grafana setup finished! **

grafana 16:00:31.23 INFO  ==> ** Starting Grafana **
Error checking server process execution privilege. error: could not get current OS user to detect process privileges
logger=settings t=2025-01-14T16:00:32.029749828Z level=info msg="Starting Grafana" version=11.3.1 commit=b5870186 branch=HEAD compiled=2024-12-04T21:19:30Z
logger=settings t=2025-01-14T16:00:32.030106833Z level=info msg="Config loaded from" file=/opt/bitnami/grafana/conf/defaults.ini
logger=settings t=2025-01-14T16:00:32.030121741Z level=info msg="Config loaded from" file=/opt/bitnami/grafana/conf/grafana.ini
logger=settings t=2025-01-14T16:00:32.030127853Z level=info msg="Config overridden from command line" arg="default.paths.data=/opt/bitnami/grafana/data"
logger=settings t=2025-01-14T16:00:32.030133252Z level=info msg="Config overridden from command line" arg="default.paths.logs=/opt/bitnami/grafana/logs"
logger=settings t=2025-01-14T16:00:32.030139785Z level=info msg="Config overridden from command line" arg="default.paths.plugins=/opt/bitnami/grafana/data/plugins"
logger=settings t=2025-01-14T16:00:32.030145165Z level=info msg="Config overridden from command line" arg="default.paths.provisioning=/opt/bitnami/grafana/conf/provisioning"
logger=settings t=2025-01-14T16:00:32.030151276Z level=info msg="Config overridden from command line" arg="default.log.mode=console"
logger=settings t=2025-01-14T16:00:32.030156807Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_DATA=/opt/bitnami/grafana/data"
logger=settings t=2025-01-14T16:00:32.030162226Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_LOGS=/opt/bitnami/grafana/logs"
logger=settings t=2025-01-14T16:00:32.030167006Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PLUGINS=/opt/bitnami/grafana/data/plugins"
logger=settings t=2025-01-14T16:00:32.030171955Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PROVISIONING=/opt/bitnami/grafana/conf/provisioning"
logger=settings t=2025-01-14T16:00:32.030176954Z level=info msg="Config overridden from Environment variable" var="GF_SECURITY_ADMIN_USER=admin"
logger=settings t=2025-01-14T16:00:32.030181703Z level=info msg="Config overridden from Environment variable" var="GF_SECURITY_ADMIN_PASSWORD=*********"
logger=settings t=2025-01-14T16:00:32.030187293Z level=info msg="Config overridden from Environment variable" var="GF_AUTH_LDAP_ENABLED=false"
logger=settings t=2025-01-14T16:00:32.030192683Z level=info msg="Config overridden from Environment variable" var="GF_AUTH_LDAP_CONFIG_FILE=/opt/bitnami/grafana/conf/ldap.toml"
logger=settings t=2025-01-14T16:00:32.030197833Z level=info msg="Config overridden from Environment variable" var="GF_AUTH_LDAP_ALLOW_SIGN_UP=false"
logger=settings t=2025-01-14T16:00:32.030202912Z level=info msg=Target target=[all]
logger=settings t=2025-01-14T16:00:32.030223912Z level=info msg="Path Home" path=/opt/bitnami/grafana
logger=settings t=2025-01-14T16:00:32.030229161Z level=info msg="Path Data" path=/opt/bitnami/grafana/data
logger=settings t=2025-01-14T16:00:32.03023404Z level=info msg="Path Logs" path=/opt/bitnami/grafana/logs
logger=settings t=2025-01-14T16:00:32.030238759Z level=info msg="Path Plugins" path=/opt/bitnami/grafana/data/plugins
logger=settings t=2025-01-14T16:00:32.030255981Z level=info msg="Path Provisioning" path=/opt/bitnami/grafana/conf/provisioning
logger=settings t=2025-01-14T16:00:32.030261592Z level=info msg="App mode production"
logger=featuremgmt t=2025-01-14T16:00:32.030644796Z level=info msg=FeatureToggles transformationsVariableSupport=true nestedFolders=true kubernetesPlaylists=true addFieldFromCalculationStatFunctions=true exploreMetrics=true lokiQuerySplitting=true openSearchBackendFlowEnabled=true logRowsPopoverMenu=true recordedQueriesMulti=true formatString=true dashboardSceneSolo=true transformationsRedesign=true lokiMetricDataplane=true prometheusAzureOverrideAudience=true notificationBanner=true dataplaneFrontendFallback=true dashgpt=true accessControlOnCall=true alertingSimplifiedRouting=true angularDeprecationUI=true lokiQueryHints=true alertingNoDataErrorExecution=true cloudWatchRoundUpEndTime=true correlations=true prometheusConfigOverhaulAuth=true tlsMemcached=true dashboardScene=true cloudWatchCrossAccountQuerying=true logsExploreTableVisualisation=true prometheusMetricEncyclopedia=true panelMonitoring=true annotationPermissionUpdate=true groupToNestedTableTransformation=true awsAsyncQueryCaching=true logsInfiniteScrolling=true dashboardSceneForViewers=true autoMigrateXYChartPanel=true influxdbBackendMigration=true cloudWatchNewLabelParsing=true logsContextDatasourceUi=true managedPluginsInstall=true topnav=true ssoSettingsApi=true pinNavItems=true promQLScope=true recoveryThreshold=true publicDashboardsScene=true publicDashboards=true alertingInsights=true lokiStructuredMetadata=true
logger=sqlstore t=2025-01-14T16:00:32.030708725Z level=info msg="Connecting to DB" dbtype=sqlite3
logger=migrator t=2025-01-14T16:00:32.033302769Z level=info msg="Locking database"
logger=migrator t=2025-01-14T16:00:32.033317487Z level=info msg="Starting DB migrations"
logger=migrator t=2025-01-14T16:00:32.122162382Z level=info msg="migrations completed" performed=0 skipped=611 duration=759.766µs
logger=migrator t=2025-01-14T16:00:32.122486957Z level=info msg="Unlocking database"
logger=secrets t=2025-01-14T16:00:32.123209303Z level=info msg="Envelope encryption state" enabled=true currentprovider=secretKey.v1
logger=plugin.angulardetectorsprovider.dynamic t=2025-01-14T16:00:32.637369269Z level=info msg="Restored cache from database" duration=403.1µs
logger=plugin.store t=2025-01-14T16:00:32.638407123Z level=info msg="Loading plugins..."
logger=plugins.registration t=2025-01-14T16:00:33.19682227Z level=error msg="Could not register plugin" pluginId=xychart error="plugin xychart is already registered"
logger=plugins.initialization t=2025-01-14T16:00:33.196863817Z level=error msg="Could not initialize plugin" pluginId=xychart error="plugin xychart is already registered"
logger=local.finder t=2025-01-14T16:00:33.196938897Z level=warn msg="Skipping finding plugins as directory does not exist" path=/opt/bitnami/grafana/plugins-bundled
logger=plugins.validator.angular t=2025-01-14T16:00:34.021641885Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=agenty-flowcharting-panel
logger=plugins.validation t=2025-01-14T16:00:34.02166578Z level=error msg="Plugin validation failed" pluginId=agenty-flowcharting-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:34.132876234Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=briangann-datatable-panel
logger=plugins.validation t=2025-01-14T16:00:34.132899778Z level=error msg="Plugin validation failed" pluginId=briangann-datatable-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:34.295320874Z level=info msg="Plugin registered" pluginId=briangann-gauge-panel
logger=plugins.validator.angular t=2025-01-14T16:00:34.323469892Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=digiapulssi-organisations-panel
logger=plugins.validation t=2025-01-14T16:00:34.323491803Z level=error msg="Plugin validation failed" pluginId=digiapulssi-organisations-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:34.600538459Z level=info msg="Plugin registered" pluginId=grafana-clock-panel
logger=plugins.validator.angular t=2025-01-14T16:00:34.70174463Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=grafana-piechart-panel
logger=plugins.validation t=2025-01-14T16:00:34.701765288Z level=error msg="Plugin validation failed" pluginId=grafana-piechart-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:35.001882939Z level=info msg="Plugin registered" pluginId=grafana-polystat-panel
logger=plugins.registration t=2025-01-14T16:00:35.49856237Z level=info msg="Plugin registered" pluginId=jdbranham-diagram-panel
logger=plugins.registration t=2025-01-14T16:00:35.597056177Z level=info msg="Plugin registered" pluginId=larona-epict-panel
logger=plugins.validator.angular t=2025-01-14T16:00:35.73622112Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=marcuscalidus-svg-panel
logger=plugins.validation t=2025-01-14T16:00:35.736243672Z level=error msg="Plugin validation failed" pluginId=marcuscalidus-svg-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:35.830448876Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=michaeldmoore-annunciator-panel
logger=plugins.validation t=2025-01-14T16:00:35.830485154Z level=error msg="Plugin validation failed" pluginId=michaeldmoore-annunciator-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:35.928717043Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=michaeldmoore-multistat-panel
logger=plugins.validation t=2025-01-14T16:00:35.928745265Z level=error msg="Plugin validation failed" pluginId=michaeldmoore-multistat-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:36.020466322Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=natel-discrete-panel
logger=plugins.validation t=2025-01-14T16:00:36.020488534Z level=error msg="Plugin validation failed" pluginId=natel-discrete-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:36.120128365Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=neocat-cal-heatmap-panel
logger=plugins.validation t=2025-01-14T16:00:36.120149975Z level=error msg="Plugin validation failed" pluginId=neocat-cal-heatmap-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:36.420121636Z level=info msg="Plugin registered" pluginId=pierosavi-imageit-panel
logger=plugins.registration t=2025-01-14T16:00:36.571112277Z level=info msg="Plugin registered" pluginId=scadavis-synoptic-panel
logger=plugins.validator.angular t=2025-01-14T16:00:36.700613709Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=snuids-trafficlights-panel
logger=plugins.validation t=2025-01-14T16:00:36.700639848Z level=error msg="Plugin validation failed" pluginId=snuids-trafficlights-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:36.722352062Z level=info msg="Plugin registered" pluginId=vonage-status-panel
logger=plugin.store t=2025-01-14T16:00:36.722374314Z level=info msg="Plugins loaded" count=62 duration=4.083968452s
logger=query_data t=2025-01-14T16:00:36.901390134Z level=info msg="Query Service initialization"
logger=live.push_http t=2025-01-14T16:00:36.921457673Z level=info msg="Live Push Gateway initialization"
logger=ngalert.notifier.alertmanager org=1 t=2025-01-14T16:00:36.994533462Z level=info msg="Applying new configuration to Alertmanager" configHash=d2c56faca6af2a5772ff4253222f7386
logger=ngalert.state.manager t=2025-01-14T16:00:37.000820206Z level=info msg="Running in alternative execution of Error/NoData mode"
logger=infra.usagestats.collector t=2025-01-14T16:00:37.002709539Z level=info msg="registering usage stat providers" usageStatsProvidersLen=2
logger=server t=2025-01-14T16:00:37.002933306Z level=info msg="Writing PID file" path=/opt/bitnami/grafana/tmp/grafana.pid pid=1
logger=provisioning t=2025-01-14T16:00:37.003500393Z level=error msg="Failed to provision data sources" error="Datasource provisioning error: yaml: unmarshal errors:\n  line 1: cannot unmarshal !!str `v1` into int64"
logger=provisioning t=2025-01-14T16:00:37.003515922Z level=error msg="Failed to provision data sources" error="Datasource provisioning error: yaml: unmarshal errors:\n  line 1: cannot unmarshal !!str `v1` into int64"
Error: ✗ Datasource provisioning error: yaml: unmarshal errors:
  line 1: cannot unmarshal !!str `v1` into int64
@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main

---

I see the issue in the logs. The main error is with the datasource provisioning:

```
Error: ✗ Datasource provisioning error: yaml: unmarshal errors:
  line 1: cannot unmarshal !!str `v1` into int64
```

This is happening because the `apiVersion` in your datasource configuration is incorrectly formatted. Let me provide the correct datasource secret configuration:



```yaml
apiVersion: v1
kind: Secret
metadata:
  name: grafana-datasource-secret
  namespace: grafana-monitoring
type: Opaque
stringData:
  datasource.yaml: |-
    apiVersion: 1
    deleteDatasources:
      - name: Prometheus
        orgId: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://prometheus:9090
        isDefault: true
        version: 1
        editable: true
        jsonData:
          timeInterval: "5s"
          httpMethod: "POST"
        secureJsonData:
          basicAuthPassword: "password"
          basicAuthUser: "user"
          tlsSkipVerify: true
      - name: Random Walk
        type: grafana-random-walk
        access: proxy
        url: http://random-walk:8080
        isDefault: false
        version: 1
        editable: true
        jsonData:
          timeInterval: "5s"
        secureJsonData:
          basicAuthPassword: "password"
          basicAuthUser: "user"
          tlsSkipVerify: true

```

Key changes made:
1. Used `stringData` instead of `data` (no base64 encoding needed)
2. Fixed the `apiVersion` format in the datasource.yaml
3. Added proper structure for datasource configuration
4. Added `version` and `editable` fields
5. Properly formatted the JSON data sections

Apply the new configuration:

```bash
kubectl apply -f grafana-datasource-secret.yaml
```

Then delete the pod to force a restart:

```bash
kubectl delete pod grafana-6fbbccf49f-hbnr4 -n grafana-monitoring
```

The new pod should come up without the datasource provisioning error. If you continue to see issues, please share the new logs after applying these changes.

Note: Make sure your Prometheus service is actually running at `http://prometheus:9090` in your cluster. If it's in a different namespace or has a different service name, you'll need to adjust the URL accordingly.

Would you like me to explain any part of the configuration in more detail?